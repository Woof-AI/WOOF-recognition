<div align="center">

# **WOOF AI** <!-- omit in toc -->

### The first AI meme project on TAO <!-- omit in toc -->
![hero](./assets/title.png)

[Dashboard](https://subnet.chunking.com) • [VectorChat](https://vectorchat.ai) • [Chunking.com](https://chunking.com) • [Toffee](https://medium.com/@vectorchat/introducing-toffee-a-new-era-in-conversational-ai-cfd09c6648ae) • [Lucid](https://github.com/VectorChat/lucid)

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

</div>
# Introduction

Welcome to the Chunking subnet, Subnet 56 on the Bittensor Protocol! This subnet is designed to advance the field of Retrieval-Augmented Generation (RAG) by incentivizing the development and service of sophisticated chunking solutions. Specifically, the subnet aims to create, host, and serve an intelligent chunking solution that maximizes intrachunk similarity and interchunk dissimilarity.

Explore our subnet [pitch deck]().

Learn more about our project at [woof ai.io]()

_See visualizations of subnet data at [subnet.woof ai.io]()_

_See how organic queries are handled [here](.)._

# Overview


# Dog Barking Sound Recognition System
This system is designed to demonstrate how to accurately recognize dog barking sounds in audio data using advanced audio recognition techniques. The system adopts the most cutting-edge algorithms in the field of deep learning and is optimized for the characteristics of dog barking, which can efficiently and accurately recognize dog barking in complex environments.


### core algorithm
### Audio Feature Extraction
VGGish + STFT - First, the audio data is feature extracted using the VGGish model, which is a CNN model pre-trained on large-scale audio datasets that extracts high-dimensional Mel-spectrogram features of the audio and captures the spectral information of the audio. - The raw audio signal is then converted into a spectrogram using a Short-Time Fourier Transform (STFT). These spectrograms are used as inputs to the Transformer model, which can fully display the time and frequency domain features of the audio signal and provide important input data for subsequent audio classification.
### Audio Classification
Audio Spectrogram Transformer (AST) for Audio Classification - AST is a specialized Transformer model designed for audio data, which employs Self-Attention mechanism to process spectrograms. This Self-Attention mechanism can efficiently capture long time-series dependencies and global features in audio signals, which has obvious advantages over traditional CNN + LSTM when processing complex and long time-series data. - In the task of classifying the sound of a barking dog, the AST model can accurately capture features such as rhythm, pitch, and mutation points in the audio, and thus effectively distinguish the dog's barking from other sounds (e.g., ambient noise or other animal barks).

## Classification of sound events
Barking vs. Non-Barking - After feature extraction and global modeling, the features generated by the Transformer model are fed into a binary classification model, which ultimately classifies the audio signal as “barking” or “non-barking”. Thanks to Transformer's powerful global modeling capabilities, the system is able to recognize barking sounds consistently and efficiently in complex audio environments.

![image/png](https://cdn-uploads.huggingface.co/production/uploads/673d82cc1898a8cd00977d97/Dy5lkdz04GLUg0MHirnra.png)

### Global dependency modeling capabilities
Transformer's self-attention mechanism allows it to capture long-distance dependencies between each time step of the input audio. This is important for audio signals such as the sound of a barking dog, which often has specific rhythmic, pitch and timing variations. Transformer is able to capture these long temporal features more efficiently and accurately than traditional LSTM models.

### Efficient training and reasoning speed
The Transformer model has a powerful parallel computing capability, which makes the training process more efficient. Compared with traditional recurrent neural networks such as LSTM, the Transformer model does not rely on the sequential processing of time steps, but calculates the data of all time steps at the same time, which improves the training speed and inference efficiency, and the training time of the Transformer model is usually shorter, especially when dealing with large-scale datasets, and the time required for training can be significantly reduced by efficient parallel computing. time.

### Excellent audio classification performance
Unlike CNN + LSTM, Transformer is able to capture both local and global features of audio through the self-attention mechanism, and is able to effectively handle long audio sequences. It is particularly good at classifying complex audio events (e.g., dog barking), and can accurately recognize and distinguish between different types of sound events.

## Advantages of pre-trained models
Pre-trained models based on Audio Spectrogram Transformer (AST) have achieved excellent results in several audio classification tasks. Through migration learning, the AST model can be quickly adapted to the dog barking sound recognition task to further improve the accuracy.

### Robustness
Transformer's ability to model global information and long temporal dependencies makes its recognition performance more stable in complex environments. In noisy backgrounds (e.g., traffic noise, environmental noise), the Transformer model still maintains high accuracy, while the traditional LSTM model is often prone to interference, leading to a decrease in recognition accuracy.
er recommendations.

![image/png](https://cdn-uploads.huggingface.co/production/uploads/673d82cc1898a8cd00977d97/EfiBKRL35DRGcXJCDaIRx.png)

